---
title: "HW5"
author: "Xuesen Zhao"
date: "2022-11-09"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggridges)
library(purrr)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Question 1

```{r}
Q1_df = tibble(
  file_name = list.files("data/hw5_data/"),
  path = str_c("data/hw5_data/", file_name)
) %>%
  mutate(
    readin = map(path, read_csv)
  ) %>%
  unnest()
```

```{r}
tidy_df = 
  Q1_df %>% 
  mutate(
    files = str_replace(file_name, ".csv", ""),
    group = str_sub(file_name, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```


## Question 2

```{r}
homicides = read_csv("./data/homicide_data.csv") %>%
  janitor::clean_names() 
```

The raw data was collected by Washington Post on over 52,000 criminal homicides over the past decade in 50 of the largest cities in the U.S. The original data set contained `r nrow(homicides)` rows and `r ncol(homicides)` columns. The features included the case id, reported date, the first and last name of the vicitim, as well as the race, age, and gender of the victim. It also contained variables that describe the location of homicides, and the disposition. 

```{r}
homicides %>%
  mutate(
    city_state = str_c(city,state,sep= ", ")
  ) %>%
  group_by(city_state) %>%
  summarize(
    solved_cases = sum(disposition == "Closed by arrest"),
    unsolved_cases = sum(disposition != "Closed by arrest")
  )
```

The above table showed the number of solved homicides (with the disposition being "Closed with arrest") and the number of unsolved homicides (with the dispositions of "Closed without arrest" or "Open/No arrest") by each city. 

```{r}
Baltimore_summary = homicides %>%
  mutate(
    city_state = str_c(city,", ",state)
  ) %>%
  group_by(city_state) %>%
  summarize(
    solved_cases = sum(disposition == "Closed by arrest"),
    unsolved_cases = sum(disposition != "Closed by arrest"),
    n = n()
  ) %>%
  filter(city_state == "Baltimore, MD") 

Baltimore_test = prop.test(
  x = Baltimore_summary %>% pull(unsolved_cases),
  n = Baltimore_summary %>% pull(n)
          ) 
  
broom::tidy(Baltimore_test) %>% 
  select(estimate,conf.low,conf.high)
```

The estimated proportion of unsolved cases is `r broom::tidy(Baltimore_test) %>% pull(estimate)` and the confidence interval is (`r broom::tidy(Baltimore_test) %>%pull(conf.low)`,`r broom::tidy(Baltimore_test) %>%pull(conf.high)`).

```{r}
unsolved_all = homicides %>%
  mutate(
    city_state = str_c(city,", ",state)
  ) %>%
  group_by(city_state) %>%
  filter(city_state != "Tulsa, AL") %>%
  summarize(
    unsolved_cases = sum(disposition != "Closed by arrest"),
    city_n = n()
  ) %>%
  mutate(
    prop_test = map2(.x = unsolved_cases, .y = city_n, ~prop.test(x = .x, n= .y)),
    tidy_test = map(.x = prop_test, ~broom::tidy(.x))
  ) %>%
  unnest(tidy_test) %>%
  select(city_state,estimate,conf.low,conf.high)
```


```{r}
ggplot(unsolved_all, aes(x=reorder(city_state, +estimate), y = estimate, fill = estimate))+ geom_bar(stat="identity")+geom_errorbar(aes(ymin = conf.low, ymax=conf.high))+labs(title = "The estimated proportions of unsolved cases for each city", x = "City & State", y = "Proportion estimate")+coord_flip()+scale_fill_gradient(low="blue",high="red")
```

Since Tulsa, AL had only 1 case, which was a solved case. We excluded it from the proportion test. Moreover, there is no city in Alabama that is called Tulsa, so it is probably a misinformation that should be removed.

## Question 3


```{r}
sim_result_df = 
  expand_grid(
  sample_size = 30,
  iteration = 1:5000,
  mu = 0:6
) %>%
  mutate(
    norm_df = map2(.x = sample_size, .y = mu, ~rnorm(n = .x, mean = .y, sd = 5))
  ) %>%
  mutate(
    t_test = map(.x = norm_df, ~t.test(x = .x)),
    tidy_test = map(.x = t_test, ~broom::tidy(.x))
  ) %>%
  unnest(tidy_test) %>%
  select(iteration, sample_size, mu, estimate, p.value)
```


```{r}
sim_result_df %>%
  group_by(mu) %>%
  summarize(
    n_rejected = sum(p.value <0.05),
    n_total = n(),
    proportion = n_rejected/n_total
  ) %>% 
  ggplot(aes(x=mu,y=proportion,fill = proportion))+geom_bar(stat="identity")+labs(title="Proportion of times the null were rejected by different population means", y = "power of the test")
```

As shown by the above graph, the larger the difference between the true population mean and our null, the higher the proportion of times the null was rejected. In other words, as effect size increases, the power also increases. 

```{r}
sim_result_df %>%
  filter(p.value < 0.05) %>%
  group_by(mu) %>%
  summarize(
    estimate_average = mean(estimate)
  ) %>%
  ggplot(aes(x = mu, y = estimate_average))+geom_point()+labs(title = "Average of the estimates vs. true population means")
```

As shown in the above graph, the sample average of $\hat \mu$ across test for which the null is rejected is approximately equal to the true value of $\mu$ as $\mu$ increases beyond 3. The average of $\hat \mu$ is very different from the true population mean when $\mu$ is equal to 1 or 2. This is because the power of the test increases as the effect size increases, so we would expect a better approximation to the true value of $\mu$ based on our estimate average. 